<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs</title>

    <meta name="description" content="APIGen">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--FACEBOOK-->
    <meta property="og:image" content="./img/overview.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1196">
    <meta property="og:image:height" content="705">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://shirley-kokane.github.io/spectool_bench/"/>
    <meta property="og:title" content="SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs" />
    <meta property="og:description" content="Project page for SpecTool BenchMark." />
 
    <!-- TWITTER
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="APIGen Pipeline" />
    <meta name="twitter:description" content="Project page for APIGen Pipeline" />
    <meta name="twitter:image" content="./img/overview.jpg" /> -->

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5JBS73F70V"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5JBS73F70V');
</script>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <b><font size="+6">SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs</font></b></br> 
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li><a href="https://www.linkedin.com/in/shirley-kokane">Shirley Kokane</a></li>
                <li><a href="https://people.cs.vt.edu/mingzhu/">Ming Zhu</a></li>
                <li><a href="https://tulikaawalgaonkar.com">Tulika Awalgaonkar</a></li>
                <li><a href="https://www.linkedin.com/in/quocthai9120/">Thai Hoang</a></li>
                <li><a href="https://jianguoz.github.io/">Jianguo Zhang</a></li>
                <li><a href="https://www.linkedin.com/in/tian-lan-770b4b165/">Tian Lan</a></li>
                <li><a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a></li>
                <li><a href="https://huan-december.github.io/">Huan Wang</a></li>
                <li><a href="https://www.shelbyh.ai/">Shelby Heinecke</a></li>
                <li><a href="http://cmxiong.com/">Caiming Xiong</a></li>

                <br>
                <br>
                    <a href="https://www.salesforceairesearch.com/">
                        <image src="img/salesforce-research.png" height="40px"> 
                        Salesforce AI Research &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    </a>
                </ul>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="https://arxiv.org/pdf/2411.13547">
                    <img src="img/paper_small.png" height="60px" alt="Paper">
                    <h4><strong>Paper</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="https://huggingface.co/datasets/Salesforce/">
                    <img src="img/data.png" height="60px" alt="Data">
                    <h4><strong>Data</strong></h4>
                </a>
            </div>
            <!-- <div class="col-md-2 text-center">
                <a href="https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4">
                    <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.svg" height="60px" alt="HuggingFace Models">
                    <h4><strong>Models</strong></h4>
                </a>
            </div> -->
        </div>


        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">Abstract</h3>
                <p class="text-justify">
                    Evaluating the output of Large Language Models (LLMs) is one of the most critical aspects of building a performant compound AI system. Since the output from LLMs propagate to downstream steps, identifying LLM errors is crucial to system performance. A common task for LLMs in AI systems is tool use. While there are several benchmark environments for evaluating LLMs on this task, they typically only give a success rate without any explanation of the failure cases. To solve this problem, we introduce SpecTool, a new benchmark to identify error patterns in LLM output on tool-use tasks. Our benchmark data set comprises of queries from diverse environments that can be used to test for the presence of seven newly characterized error patterns. Using SPECTOOL , we show that even the most prominent LLMs exhibit these error patterns in their outputs. Researchers can use the analysis and insights from SPECTOOL to guide their error mitigation strategies.
                </p>
                <p style="text-align:center;">
                    <image src="img/spectool_overview.png">
                </p>

                <h3 class="mt-4 mb-2">Error Patterns in Tool Use</h3>
                <p class="text-justify">
                    This section introduces the varied metrics we have implemented to capture the brittle nature of tool calls, we characterize systematic errors generated by LLMs on tool-use tasks into seven critical error types.
                </p>
                <ul>
                    <li><strong>Insufficient API Calls (IAC):</strong> Unable to generate sufficient API calls</li>
                    <li><strong>Incorrect Argument Value (IAV):</strong> Generates incorrect argument values. This also includes exclusion of necessary arguments.</li>
                    <li><strong>Incorrect Argument Name  (IAN):</strong> Hallucinates argument names.</li>
                    <li><strong>Incorrect Argument Type (IAT):</strong> Generates incorrect argument type.</li>
                    <li><strong>Repeated API Calls (RAC):</strong> Exact same API Call repeatedly, leading to redundant calls.</li>
                    <li><strong>Incorrect Function Name (IFN):</strong> Hallucinates function names, that are not included in the API list</li>
                    <li><strong>Invalid Format Error (IFE):</strong> Invalid format instructions provided for parsing.</li>
                </ul>

                <p style="text-align:center;">
                    <image src="img/errors.png" width="100%">
                </p>

                <h3 class="mt-4 mb-2">Framework</h3>
                <p class="text-justify">
                    This section introduces the detailed design of the benchmark, an <b>Automated Pipeline for Generating verifiable and diverse function-calling datasets</b>. Our framework is designed with three key factors in mind: data quality, data diversity, and collection scalability. We achieve these through the key modules shown in the figure below: the multi-stage data verification process ensures data quality, the seed QA (query-answer) data sampler, API sampler, and various prompt templates ensure diversity, and our structured modular design using a unified format enables the system to scale to diverse API sources, including but not limited to Python functions and representational state transfer (REST) APIs.
                </p>
                <h4 class="mt-4 mb-2">SpecTool Dataset</h4>
                <p class="text-justify">
                    The process begins by gathering seed queries for each API environment, sourced from benchmark tests conducted on toolsets like TOOLBENCH (Guo et al., 2024) and AGENTBOARD (Ma et al., 2024). These initial seed queries are then augmented using the following methods.
                </p>
                <p style="text-align:center;">
                    <image src="img/data_augment.png" width="100%">
                </p>
                

                <h4 class="mt-4 mb-2">Benchmark</h4>
                <p class="text-justify">
                    In the evaluation benchmark, we ensure that the environments are deterministic. This allows the trajectory of the agents to depend solely on the policy and actions selected by the language model. Agents are given a description of the available tools and task instructions, including format requirements.
                </p>
                <h4 class="mt-4 mb-2">Feedback Mechanism</h4>
                <p class="text-justify">
                    The actions generated by the agents are reviewed by an inbuilt feedback mechanism designed to detect prominent errors. 
                </p>
                <ul>
                    <li><b>Format Validator:</b> This step verifies whether the generated action is correctly parsed and adheres to the format instructions provided. If not, we prompt the agent with the set format instructions.</li>
                    <li><b>API Action Validator:</b> This step determines if the generated action is included in the specified action space. If it is not, we enumerate the list of available actions that can be used to resolve the query.</li>
                    <li><b>Argument Validator:</b> This step assesses whether the generated action is invoked with the appropriate arguments.  If the arguments are incorrect, provide a detailed list of the available arguments for the chosen action, along with their respective descriptions.</li>
                    <li><b>Argument Type Validator:</b> This step ensures that the generated action is called with arguments of the correct type. If any discrepancies are found, specify the correct argument type required for proper execution of the function.</li>
                </ul>

                <h4 class="mt-4 mb-2">Experiments</h4>
                <p class="text-justify">
                    We perform an in-depth assessment of widely used large language models, encompassing both proprietary, API-based systems and models with publicly accessible weights.
                </p>

                <h4 class="mt-4 mb-2">Results</h4>
                <div align="center">
                    <img src="img/results.png" width="620" alt="Performance comparison on SpecTool Benchmark">
                    <p>Error evaluation on the SpecTool Benchmark. </p>
                </div>
                
                <h4 class="mt-4 mb-2">Ablation Study</h4>
                <p class="text-justify">
                    We performed an ablation study to assess the importance of the constructive feedback mechanism introduced in the benchmark. As shown below, the feedback mechanism plays a pivotal role in enabling the model to refine its actions by concentrating on essential aspects, such as correctly identifying API names, selecting appropriate API arguments, and adhering to the expected data types for those arguments.
                </p>
                
                <p class="text-justify">
                    We also conducted an in-depth examination of a subset of queries drawn from two distinct API environments to investigate the propensity of models to generate fabricated API calls when presented with unexpected or irrelevant requests.
                </p>

                <div style="display: flex; justify-content: space-between; align-items: center;">
                    <img src="./img/feedback_plot.png" alt="Feedback Ablation" style="width: 48%; height: auto;">
                    <img src="./img/irrelevance_plot.png" alt="Irrelevance Ablation" style="width: 48%; height: auto;">
                </div>
                
                <h4 class="mt-4 mb-2">Conclusion</h4>
                <p class="text-justify">
                    Our findings highlight the importance of task-specific fine-tuning, as models optimized for API calls perform more reliably and reduce hallucinations compared to chat-oriented models.
                </p>

            </div>
        </div>
    </div>
    <div class="row justify-content-md-center mt-4">
        <div class="col-md-10 col-lg-8">
            <p class="text-justify" style="font-size: 10px;">
                This page is adapted from the template of <a href="https://video-language-planning.github.io/">Video Language Planning</a> project website. We thank the authors for providing the template.
            </p>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
