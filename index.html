<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs</title>

    <meta name="description" content="APIGen">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--FACEBOOK-->
    <meta property="og:image" content="./img/overview.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1196">
    <meta property="og:image:height" content="705">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://shirley-kokane.github.io/spectool_bench/"/>
    <meta property="og:title" content="SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs" />
    <meta property="og:description" content="Project page for SpecTool BenchMark." />
 
    <!-- TWITTER
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="APIGen Pipeline" />
    <meta name="twitter:description" content="Project page for APIGen Pipeline" />
    <meta name="twitter:image" content="./img/overview.jpg" /> -->

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5JBS73F70V"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5JBS73F70V');
</script>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <b><font size="+6">APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets</font></b></br> 
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li><a href="https://www.linkedin.com/in/shirley-kokane">Shirley Kokane</a></li>
                <li><a href="https://people.cs.vt.edu/mingzhu/">Ming Zhu</a></li>
                <li><a href="https://tulikaawalgaonkar.com">Tulika Awalgaonkar</a></li>
                <li><a href="https://www.linkedin.com/in/quocthai9120/">Thai Hoang</a></li>
                <li><a href="https://jianguoz.github.io/">Jianguo Zhang</a></li>
                <li><a href="https://www.linkedin.com/in/tian-lan-770b4b165/">Tian Lan</a></li>
                <li><a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a></li>
                <li><a href="https://huan-december.github.io/">Huan Wang</a></li>
                <li><a href="https://www.shelbyh.ai/">Shelby Heinecke</a></li>
                <li><a href="http://cmxiong.com/">Caiming Xiong</a></li>

                <br>
                <br>
                    <a href="https://www.salesforceairesearch.com/">
                        <image src="img/salesforce-research.png" height="40px"> 
                        Salesforce AI Research &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    </a>
                </ul>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="https://arxiv.org/pdf/2411.13547">
                    <img src="img/paper_small.png" height="60px" alt="Paper">
                    <h4><strong>Paper</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="https://huggingface.co/datasets/Salesforce/">
                    <img src="img/data.png" height="60px" alt="Data">
                    <h4><strong>Data</strong></h4>
                </a>
            </div>
            <!-- <div class="col-md-2 text-center">
                <a href="https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4">
                    <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.svg" height="60px" alt="HuggingFace Models">
                    <h4><strong>Models</strong></h4>
                </a>
            </div> -->
        </div>


        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">Abstract</h3>
                <p class="text-justify">
                    Evaluating the output of Large Language Models (LLMs) is one of the most critical aspects of building a performant compound AI system. Since the output from LLMs propagate to downstream steps, identifying LLM errors is crucial to system performance. A common task for LLMs in AI systems is tool use. While there are several benchmark environments for evaluating LLMs on this task, they typically only give a success rate without any explanation of the failure cases. To solve this problem, we introduce SpecTool, a new benchmark to identify error patterns in LLM output on tool-use tasks. Our benchmark data set comprises of queries from diverse environments that can be used to test for the presence of seven newly characterized error patterns. Using SPECTOOL , we show that even the most prominent LLMs exhibit these error patterns in their outputs. Researchers can use the analysis and insights from SPECTOOL to guide their error mitigation strategies.
                </p>
                <p style="text-align:center;">
                    <image src="img/spectool_overview.jpg" width="100%">
                </p>

                <h3 class="mt-4 mb-2">Error Patterns in Tool Use</h3>
                <p class="text-justify">
                    This section introduces the varied metrics we have implemented to capture the brittle nature of tool calls, we characterize systematic errors generated by LLMs on tool-use tasks into seven critical error types.
                </p>
                <ul>
                    <li><strong>Insufficient API Calls (IAC):</strong> Unable to generate sufficient API calls</li>
                    <li><strong>Incorrect Argument Value (IAV):</strong> Generates incorrect argument values. This also includes exclusion of necessary arguments.</li>
                    <li><strong>Incorrect Argument Name  (IAN):</strong> Hallucinates argument names.</li>
                    <li><strong>Incorrect Argument Type (IAT):</strong> Generates incorrect argument type.</li>
                    <li><strong>Repeated API Calls (RAC):</strong> Exact same API Call repeatedly, leading to redundant calls.</li>
                    <li><strong>Incorrect Function Name (IFN):</strong> Hallucinates function names, that are not included in the API list</li>
                    <li><strong>Invalid Format Error (IFE):</strong> Invalid format instructions provided for parsing.</li>
                </ul>

                <h3 class="mt-4 mb-2">Framework</h3>
                <p class="text-justify">
                    This section introduces the detailed design of the benchmark, an <b>Automated Pipeline for Generating verifiable and diverse function-calling datasets</b>. Our framework is designed with three key factors in mind: data quality, data diversity, and collection scalability. We achieve these through the key modules shown in the figure below: the multi-stage data verification process ensures data quality, the seed QA (query-answer) data sampler, API sampler, and various prompt templates ensure diversity, and our structured modular design using a unified format enables the system to scale to diverse API sources, including but not limited to Python functions and representational state transfer (REST) APIs.
                </p>
                <h4 class="mt-4 mb-2">SpecTool Dataset</h4>
                <p class="text-justify">
                    The process begins by gathering seed queries for each API environment, sourced from benchmark tests conducted on toolsets like TOOLBENCH (Guo et al., 2024) and AGENTBOARD (Ma et al., 2024). These initial seed queries are then augmented using the following methods.
                </p>
                <p style="text-align:center;">
                    <image src="img/data_augment.png" width="100%">
                </p>
                

                <h4 class="mt-4 mb-2">Benchmark</h4>
                <p class="text-justify">
                    In the evaluation benchmark, we ensure that the environments are deterministic. This allows the trajectory of the agents to depend solely on the policy and actions selected by the language model. Agents are given a description of the available tools and task instructions, including format requirements.
                </p>
                <h4 class="mt-4 mb-2">Feedback Mechanism</h4>
                <p class="text-justify">
                    The actions generated by the agents are reviewed by an inbuilt feedback mechanism designed to detect prominent errors. 
                </p>
                <ul>
                    <li><b>Format Validator:</b> This step verifies whether the generated action is correctly parsed and adheres to the format instructions provided</li>
                    <li><b>API Action Validator:</b> </li>
                    <li><b>Argument Validator:</b> </li>
                    <li><b>Argument Type Validator:</b> </li>
                </ul>

                <h4 class="mt-4 mb-2">Experiments</h4>
                <p class="text-justify">
                    Encouraging diversity in training datasets is crucial for developing robust function-calling agents that can handle a wide range of real-world scenarios. In APIGen, we promote data diversity through multiple perspectives, including query style diversity, sampling diversity, and API diversity.
                </p>

                <h4 class="mt-4 mb-2">Dataset API Sources</h4>
                <p class="text-justify">
                    To ensure a high-quality and diverse dataset, we focused on collecting real-world APIs that could be readily executed and came with thorough documentation. We primarily sourced APIs from ToolBench, a comprehensive tool-use dataset that includes 16,464 REST APIs across 49 coarse-grained categories from RapidAPI Hub. This hub is a leading marketplace featuring a vast array of developer-contributed APIs.
                </p>
                
                
                <p class="text-justify">
                    The semantic checker also plays a crucial role in filtering generated data that does not align with the query's objectives. For instance, if a user's query contains multiple requests, but the returned results only address one, or if the generated function-call data and execution results do not match the user's query, the data point will be filtered out. Including these data points in the training set for model training could potentially harm the performance, as demonstrated in the experiments.
                </p>
                
                <p class="text-justify">
                    We are releasing approximately 60,000 high-quality function-calling datasets generated from the two strongest models: Mixtral-8x22B-Inst and DeepSeek-V2-Chat (236B). These datasets include all the query styles mentioned and cover a wide range of practical situations, with 3,673 diverse APIs across 21 categories. Each data point has been verified using real-world APIs to ensure its validity and usefulness. By making this dataset publicly available, we aim to benefit the research community and facilitate future work in this area.
                </p>

                <p class="text-justify">
                    We mainly test our function-calling models on the <a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard (BFCL)</a>, which offers a comprehensive evaluation framework for assessing LLMs' function-calling capabilities across various programming languages and application domains like Java, JavaScript, and Python.
                </p>
                
                <div align="center">
                    <img src="https://github.com/apigen-pipeline/apigen-pipeline.github.io/blob/main/img/table-result-0718.png?raw=true" width="620" alt="Performance comparison on Berkeley Function-Calling Leaderboard">
                    <p>Performance comparison on the BFCL benchmark as of date 07/18/2024. Evaluated with <code>temperature=0.001</code> and <code>top_p=1</code></p>
                </div>
                
                <p class="text-justify">
                    Our <code>xLAM-7b-fc-r</code> secures the 3rd place with an overall accuracy of 88.24% on the leaderboard, outperforming many strong models. Notably, our <code>xLAM-1b-fc-r</code> model is the only tiny model with less than 2B parameters on the leaderboard, but still achieves a competitive overall accuracy of 78.94%, outperforming GPT3-Turbo and many larger models. Both models exhibit balanced performance across various categories, showing their strong function-calling capabilities despite their small sizes.
                </p>

            </div>
        </div>
    </div>
    <div class="row justify-content-md-center mt-4">
        <div class="col-md-10 col-lg-8">
            <p class="text-justify" style="font-size: 10px;">
                This page is adapted from the template of <a href="https://video-language-planning.github.io/">Video Language Planning</a> project website. We thank the authors for providing the template.
            </p>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
